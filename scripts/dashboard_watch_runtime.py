# scripts/dashboard_watch_runtime.py
# Owlume — T5-S7/T5-S8 Runtime Feedback + Snapshot Watcher
# Stage 5 Final (2025-10-31)
#
# Watches runtime feedback logs → updates dashboard aggregates →
# computes clarity balance → writes daily snapshot reports.

import os, sys, time, json, glob, argparse
from datetime import datetime, timezone
from pathlib import Path

# --- Directories -------------------------------------------------------------

ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = ROOT / "data"
RUNTIME_DIR = DATA_DIR / "runtime"
METRICS_DIR = DATA_DIR / "metrics"
REPORTS_DIR = ROOT / "reports"
SCRIPTS_DIR = ROOT / "scripts"
STATE_FILE = RUNTIME_DIR / "watch_state.json"

# --- BSE helper (non-blocking sidecar) ---
import subprocess, sys, json
from pathlib import Path

def emit_bse_update(reflection_dict: dict):
    """Fire-and-forget BSE update using STDIN (no temp files)."""
    try:
        root = Path(__file__).resolve().parents[1]
        cmd = [sys.executable, str(root / "scripts" / "update_bias_vector.py"), "--user", reflection_dict.get("user","local")]
        p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        out, err = p.communicate(json.dumps(reflection_dict))
        if out:
            print("[BSE]", out.strip())
        if err:
            print("[BSE][WARN]", err.strip())
    except Exception as e:
        print("[BSE][ERROR]", e)
# --- end BSE helper ---

# --- Heartbeat vitals helper ---

def _latest(path_glob: str):
    files = sorted(Path().glob(path_glob), key=lambda p: p.stat().st_mtime if p.exists() else 0)
    return files[-1] if files else None

def _read_json_or(path, default):
    if not path or not path.exists():
        return default
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return default

def _format_pct(x):
    try:
        return f"{float(x)*100:.1f}%"
    except Exception:
        return "—"

def make_heartbeat_text():
    ts = datetime.now(timezone.utc).isoformat(timespec="seconds")

    agg = _read_json_or(_latest("data/metrics/aggregates_*.json"), {})
    avg_delta = (agg.get("clarity", {}) or {}).get("avg_delta", agg.get("avg_delta"))
    empathy = (agg.get("empathy", {}) or {}).get("activation_rate", agg.get("empathy_activation"))
    positive = (agg.get("clarity", {}) or {}).get("positive_rate", agg.get("positive_rate"))

    bal = _read_json_or(_latest("data/metrics/aggregates_balance_*.json"), {})
    depth = bal.get("depth_score")
    breadth = bal.get("breadth_score")
    drift = bal.get("drift_index")
    tunnel = bal.get("tunnel_index")

    aim = "neutral"
    try:
        d = float(breadth or 0) - float(depth or 0)
        aim = "structure" if d > 0.25 else ("widen" if d < -0.25 else "neutral")
    except Exception:
        pass

    delta_str = f"{float(avg_delta):.3f}" if isinstance(avg_delta, (int, float)) else "—"
    empathy_str = _format_pct(empathy)
    positive_str = _format_pct(positive)
    drift_str = f"{float(drift):.3f}" if isinstance(drift, (int, float)) else "—"
    tunnel_str = f"{float(tunnel):.3f}" if isinstance(tunnel, (int, float)) else "—"

    return (
        "Stage: 5 — Activation Phase\n"
        "Milestone: T5-S8 — Runtime Loop Operational\n"
        f"Last Snapshot: {ts}\n"
        f"Vitals: Δavg={delta_str} | Empathy={empathy_str} | Positive={positive_str} | "
        f"Drift={drift_str} | Tunnel={tunnel_str} | Aim={aim}\n"
        "Generated by dashboard_watch_runtime.py\n"
    )

# --- Utilities ---------------------------------------------------------------

def load_state(path: Path):
    if not path.exists():
        return {"processed_ids": [], "last_seen_ts": None}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"processed_ids": [], "last_seen_ts": None}


def save_state(path: Path, state: dict):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def get_runtime_events() -> list:
    """Load events from runtime insight log."""
    events = []
    path = RUNTIME_DIR / "insight_events.jsonl"
    if not path.exists():
        return events
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                ev = json.loads(line)
                events.append(ev)
            except Exception:
                continue
    return events


def detect_new_events(events, processed_ids, last_seen_ts):
    """Return subset of new events since last state."""
    new = []
    for ev in events:
        eid = ev.get("did") or ev.get("id")
        ts = ev.get("timestamp")
        if eid in processed_ids:
            continue
        if last_seen_ts and ts and ts <= last_seen_ts:
            continue
        new.append(ev)

    # --- BSE sidecar: emit once per newly detected event ---
    try:
        for _ev in new:
            reflection = {
                "user": _ev.get("user", "local"),
                "did": _ev.get("did") or _ev.get("id"),
                "timestamp": _ev.get("timestamp"),
                "cg_delta": _ev.get("cg_delta", ((_ev.get("cg_post", 0.0) or 0.0) - (_ev.get("cg_pre", 0.0) or 0.0))),
                "empathy_state": _ev.get("empathy_state", {}),
                "diagnostics": _ev.get("diagnostics", {})
            }
            emit_bse_update(reflection)
    except Exception as e:
        print("[BSE][ERROR detect_new_events]", e)
    # --- end BSE sidecar ---

    return new


def merge_feedback_into_aggregates():
    """Placeholder: in production this merges runtime feedback."""
    agg_files = sorted(glob.glob(str(METRICS_DIR / "aggregates_*.json")))
    if not agg_files:
        print("[T5-S7] no aggregates found to merge")
        return
    latest = agg_files[-1]
    print(f"[T5-S7] merged latest aggregates: {os.path.basename(latest)}")

def _latest(path_glob: str) -> Path | None:
    files = sorted(Path().glob(path_glob), key=lambda p: p.stat().st_mtime if p.exists() else 0)
    return files[-1] if files else None

def _read_json_or(path: Path | None, default: dict) -> dict:
    if not path or not path.exists():
        return default
    try:
        import json
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return default

def _format_pct(x):
    try:
        return f"{float(x)*100:.1f}%"
    except Exception:
        return "—"

def _make_heartbeat_text():
    from datetime import datetime, timezone
    ts = datetime.now(timezone.utc).isoformat(timespec="seconds")

    # newest regular aggregates
    agg = _read_json_or(_latest("data/metrics/aggregates_*.json"), {})
    # accept nested or flat
    avg_delta = (agg.get("clarity", {}) or {}).get("avg_delta", agg.get("avg_delta"))
    empathy = (agg.get("empathy", {}) or {}).get("activation_rate", agg.get("empathy_activation"))
    positive = (agg.get("clarity", {}) or {}).get("positive_rate", agg.get("positive_rate"))

    # newest balance aggregates
    bal = _read_json_or(_latest("data/metrics/aggregates_balance_*.json"), {})
    depth = bal.get("depth_score")
    breadth = bal.get("breadth_score")
    drift = bal.get("drift_index")
    tunnel = bal.get("tunnel_index")

    # derive a simple “aim” summary from balance
    aim = "neutral"
    try:
        d = float(breadth or 0) - float(depth or 0)
        aim = "structure" if d > 0.25 else ("widen" if d < -0.25 else "neutral")
    except Exception:
        pass

    # format
    delta_str = f"{float(avg_delta):.3f}" if isinstance(avg_delta, (int, float)) else "—"
    empathy_str = _format_pct(empathy)
    positive_str = _format_pct(positive)
    drift_str = f"{float(drift):.3f}" if isinstance(drift, (int, float)) else "—"
    tunnel_str = f"{float(tunnel):.3f}" if isinstance(tunnel, (int, float)) else "—"

    return (
        "Stage: 5 — Activation Phase\n"
        "Milestone: T5-S8 — Runtime Loop Operational\n"
        f"Last Snapshot: {ts}\n"
        f"Vitals: Δavg={delta_str} | Empathy={empathy_str} | Positive={positive_str} | Drift={drift_str} | Tunnel={tunnel_str} | Aim={aim}\n"
        "Generated by dashboard_watch_runtime.py\n"
    )

def run_balance_and_snapshot():
    """Trigger the Stage-5 balance and snapshot scripts sequentially, then Stage-6 hooks."""
    # -------- Stage 5: core balance + snapshot (safe try/except) --------
    try:
        # 1) compute & snapshot
        os.system(f"{sys.executable} -u {SCRIPTS_DIR / 'compute_balance_metrics.py'}")
        os.system(f"{sys.executable} -u {SCRIPTS_DIR / 'generate_snapshot_report.py'}")

        # 2) write heartbeat (metadata-only snapshot)
        ts = datetime.now(timezone.utc).isoformat(timespec="seconds")
        heartbeat = (
            "Stage: 5 — Activation Phase\n"
            "Milestone: T5-S8 — Runtime Loop Operational\n"
            f"Last Snapshot: {ts}\n"
            "Generated by dashboard_watch_runtime.py\n"
        )
        # If you prefer your helper, swap the 4 lines above with:
        # heartbeat = _make_heartbeat_text()

        REPORTS_DIR.mkdir(parents=True, exist_ok=True)
        with open(REPORTS_DIR / "last_snapshot.log", "w", encoding="utf-8") as f:
            f.write(heartbeat)

    except Exception as e:
        print(f"[T5-S8] snapshot heartbeat error: {e}")

    # -------- Stage 6: hooks (standalone try/except to avoid crash) --------
    try:
        import glob, json, time, subprocess
        from pathlib import Path

        def _collect_vitals():
            """
            Try sources in order:
              1) reports/summary.json (preferred if present)
              2) newest data/metrics/aggregates_*.json
            Return dict with: delta_avg, empathy_rate (0..1), positive_rate (0..1), drift, tunnel, aim
            or None if unavailable.
            """
            # 1) summary.json
            sum_p = Path("reports/summary.json")
            if sum_p.exists():
                try:
                    data = json.loads(sum_p.read_text(encoding="utf-8"))
                    v = data.get("vitals")
                    if v:
                        return {
                            "delta_avg": float(v.get("delta_avg", 0.0)),
                            "empathy_rate": float(v.get("empathy_rate", 0.0)),
                            "positive_rate": float(v.get("positive_rate", 0.0)),
                            "drift": float(v.get("drift", 0.0)),
                            "tunnel": float(v.get("tunnel", 0.0)),
                            "aim": str(v.get("aim", "structure")),
                        }
                except Exception:
                    pass

            # 2) newest aggregates_*.json
            paths = sorted(glob.glob("data/metrics/aggregates_*.json"))
            if paths:
                try:
                    with open(paths[-1], "r", encoding="utf-8") as af:
                        a = json.load(af)
                    return {
                        "delta_avg": float(a.get("avg_delta", a.get("delta_avg", 0.0))),
                        "empathy_rate": float(a.get("empathy_rate", 0.0)),
                        "positive_rate": float(a.get("positive_rate", 0.0)),
                        "drift": float(a.get("drift", 0.0)),
                        "tunnel": float(a.get("tunnel", 0.0)),
                        "aim": str(a.get("aim", "structure")),
                    }
                except Exception:
                    pass
            return None

        v = _collect_vitals()
        if v:
            # 3) append Vitals line to last_snapshot.log
            vitals_line = (
                f"Vitals: Δavg={v['delta_avg']:.3f} | "
                f"Empathy={v['empathy_rate']*100:.1f}% | "
                f"Positive={v['positive_rate']*100:.1f}% | "
                f"Drift={v['drift']:.3f} | "
                f"Tunnel={v['tunnel']:.3f} | "
                f"Aim={v['aim']}"
            )
            with open(REPORTS_DIR / "last_snapshot.log", "a", encoding="utf-8") as f:
                f.write(vitals_line + "\n")
            print(vitals_line)

            # 4) (S6-S4) export summary.json for integrations & dashboard v3
            summary = {
                "stage": "5 — Activation Phase",
                "milestone": "T5-S8 — Runtime Loop Operational",
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S+00:00", time.gmtime()),
                "vitals": v,
                "links": {
                    "last_snapshot": "last_snapshot.log",
                    "clarity_trend": "charts/clarity_trend.png",
                    "empathy_curve": "charts/empathy_curve.png",
                    "dashboard_v2": "dashboard_v2.html",
                },
            }
            with open(REPORTS_DIR / "summary.json", "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print("[S6-S4] summary.json updated ✓")

            # 5) (S6-S1) trigger Agent Nudge
            try:
                subprocess.run(
                    [sys.executable, "-u", str(SCRIPTS_DIR / "agent_nudge_from_snapshot.py"),
                     "--in_log", str(REPORTS_DIR / "last_snapshot.log"),
                     "--out_jsonl", "data/runtime/nudge_events.jsonl"],
                    check=False
                )
            except Exception as e:
                print(f"[S6-S1] Agent Nudge skipped: {e}")
        else:
            print("[S6] Vitals unavailable (summary.json & aggregates_* not found)")

    except Exception as e:
        print(f"[S6] hooks error: {e}")


def process_cycle():
    """One scan/update cycle; returns number of new events processed."""
    events = get_runtime_events()
    state = load_state(STATE_FILE)
    processed_ids = set(state.get("processed_ids", []))
    last_seen_ts = state.get("last_seen_ts")

    new_events = detect_new_events(events, processed_ids, last_seen_ts)
    n_new = len(new_events)
    if not n_new:
        return 0

    # process each new event (placeholder for aggregation logic)
    for ev in new_events:
        did = ev.get("did")
        processed_ids.add(did)
        ts = ev.get("timestamp")
        if ts and (not last_seen_ts or ts > last_seen_ts):
            last_seen_ts = ts

    # update dashboard aggregates
    merge_feedback_into_aggregates()
    run_balance_and_snapshot()

    # update state
    new_state = {"processed_ids": list(processed_ids), "last_seen_ts": last_seen_ts}
    save_state(STATE_FILE, new_state)
    print(f"[T5-S7] processed {n_new} new event(s)")
    return n_new


# --- CLI --------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Owlume Stage-5 Dashboard Watcher")
    parser.add_argument("--once", action="store_true", help="Run a single cycle then exit")
    parser.add_argument("--poll_secs", type=int, default=15, help="Polling interval in seconds")
    args = parser.parse_args()

    if args.once:
        processed = process_cycle()
        print(f"[T5-S7] once: processed={processed}")

        # Always refresh heartbeat + vitals + summary.json in --once mode
        run_balance_and_snapshot()
        return 0


    print("[T5-S7] watcher: running (Ctrl+C to stop)…")
    try:
        while True:
            processed = process_cycle()
            time.sleep(args.poll_secs)
    except KeyboardInterrupt:
        print("\n[T5-S7] watcher stopped.")
        return 0


if __name__ == "__main__":
    sys.exit(main())
