# ğŸ¦‰ Owlume â€” Golden Set Challenge (Final Version)

## Purpose  
The Golden Set Challenge proves that Owlume is **not just another AI wrapper**.  
It validates that the MVP consistently produces **provocative, precise, and human** blind-spot questions â€” not generic lists or summaries.  

This QA flow enforces Owlumeâ€™s brand promise:

> *Illuminate your blind spots before they cost you.*

---

## Structure  
Each run uses **20 short dilemmas** (2â€“4 sentences).  
Every dilemma is tagged with its expected **Mode Ã— Principle** from the Questioncraft Matrix.  
Evaluators run each dilemma through Owlume GPT and record outcomes.

---

## Evaluation Flow  

| Step | Action | Pass Criteria |
|------|---------|---------------|
| **D1 â€” Setup** | Load the `golden_set_dilemmas.csv` file. | File opens cleanly; each entry includes Mode Ã— Principle tags. |
| **D2 â€” Run Test** | Paste each dilemma into Owlume GPT. | Response returns **3â€“7 sharp questions**, with **no advice or explanation.** |
| **D3 â€” Empathy Toggle** | Re-run with â€œEmpathy ON.â€ | Language softens; structure and sharpness remain intact. |
| **D4 â€” Brand Check** | Verify tone aligns with *Front Page Flow* (human, reflective, confident). | Pass if it feels unmistakably like Owlumeâ€™s voice. |
| **D5 â€” Provocative Precision Rule** | Apply **Reject Generic Breadth / Accept Provocative Precision.** | **Fail if:** output reads like a consultantâ€™s list of categories or â€œareas to consider.â€<br>**Pass if:** it delivers 2â€“5 sharp, uncomfortable questions exposing assumptions, evidence gaps, hidden drivers, or consequences.<br>**CEO-Level Litmus Test:** Would a world-class decision-maker pause to debate this question or dismiss it as generic?<br>â†’ If **pause â†’ Pass.**  If **dismiss â†’ Fail.** |
| **D6 â€” Mode Accuracy** | Compare detected Mode to expected tag. | â‰¥ 80 % match = Pass. |
| **D7 â€” Principle Accuracy** | Compare detected Principle to expected tag. | â‰¥ 70 % match = Pass. |
| **D8 â€” Empathy Integrity** | Ensure empathy overlay didnâ€™t distort logic. | Questions remain logically sound and relevant. |
| **D9 â€” Differentiation Check** | Run same dilemma in generic AI prompt â€œWhat blind spots?â€ | Owlume output must be **sharper, more structured, more human-framed.** |
| **D10 â€” Summary Report** | Record Pass/Fail per step. | â‰¥ 85 % overall pass rate = Golden Set MVP Ready. |

---

### ğŸ”— Related Reference â€” Golden Rules Appendix  

For detailed guidance on how to evaluate Step **D5 (Provocative Precision)** â€” including rationale, real examples of *Fail vs Pass*, and dataset curation tips â€” see:

â¡ï¸ **[Golden Rules â†’ Appendix: Applying the D5 Rule in Practice](/docs/golden_rules.md)**  

This appendix expands on the philosophy behind Owlumeâ€™s sharpness standard and helps QA evaluators maintain consistent interpretation across tests.

---

## Files and Locations  

| File | Folder | Description |
|------|---------|-------------|
| `golden_set_dilemmas.csv` | `/qa/` | 20 dilemmas + expected Mode Ã— Principle. |
| `golden_set_readme.md` | `/docs/` | This protocol. |
| `golden_set_results.md` | `/qa/results/` | Log of all test outcomes with timestamps. |
| `golden_rules.md` | `/docs/` | Contains **Provocative Precision** rule + **CEO Litmus Test.** |

---

## CEO-Level Litmus Test (Embedded in Track D2)  

> **Ask:**  
> â€œWould a world-class decision-maker pause the meeting to debate this question â€” or dismiss it as generic?â€  
>  
> **If pause â†’ Pass.**  
> **If dismiss â†’ Fail.**

---

## Completion Criteria  

âœ… Golden Set Challenge = **Complete** when:  
- 20 dilemmas validated through all 10 steps.  
- â‰¥ 85 % overall Pass rate.  
- No failures in Step D5 (Provocative Precision).  
- QA log stored in `/qa/results/`.

---

## Brand Tie-In  

> **Front Page Flow â†” Golden Set**  
> Every accepted output must *sound* and *feel* like:  
> â€œOwlume reveals what youâ€™re missing â€” because what you miss costs you.â€

---

*Last updated:* **2025-10-11**  
*Status:* âœ… **Final, Locked**
